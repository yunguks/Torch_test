{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 https://github.com/FrancescoSaverioZuppichini/ViT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import einops\n",
    "from einops.layers.torch import Rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(8,3,224,224)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Embedding   \n",
    "- 이미지를 Patch로 나누는 방법 2가지   \n",
    "-- 1. einops의 rearrange   \n",
    "-- 2. Covn2d layer로 patch크기와 같은 filter를 사용   \n",
    "   \n",
    "   Batch * C * H * W --> Batch * N * (P * P * C)   \n",
    "   H * W --> N * ( P * P )   \n",
    "- 실제의 VIT에서는 einops같은 Linear Embedding 보다 Conv2d Layer로 사용한 후 Flatten 한 것이 performance gain이 있습니다   \n",
    "   \n",
    "   -- google research 에서는 conv2d 후 jax.numpy로 reshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n",
      "patches : torch.Size([8, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "# 1. einops.rearrange 함수로 patch\n",
    "# 8x3x(14*16)x(14*16) -> 8x(14*14)x(16*16*3) 으로 flatten\n",
    "patch_size = 16\n",
    "\n",
    "print(x.shape)\n",
    "patches = einops.rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n",
    "print(f'patches : {patches.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. convlayer로 patch 만들기\n",
    "patch_size = 16\n",
    "in_channels =3\n",
    "emb_size = 768\n",
    "\n",
    "projection = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, emb_size, kernel_size= patch_size,\n",
    "    stride=patch_size),\n",
    "    # einops.layers.torch.Rearrange 함수 사용\n",
    "    Rearrange('b e (h) (w) -> b(h w) e')\n",
    ")\n",
    "projection(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class_Token과 Positional Encoding 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch x shape : torch.Size([8, 196, 768])\n",
      "Class Token shape : torch.Size([1, 1, 768])\n",
      "Class Token after Repeat batch size : torch.Size([8, 1, 768])\n",
      "Position : torch.Size([197, 768])\n",
      "concat x shape : torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "emb_size = 768\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "# patch만들기\n",
    "patch_x = projection(x)\n",
    "print(f'Patch x shape : {patch_x.shape}')\n",
    "\n",
    "#class token\n",
    "cls_token = nn.Parameter(torch.randn(1,1,emb_size))\n",
    "print(f'Class Token shape : {cls_token.shape}')\n",
    "\n",
    "batch_size = 8\n",
    "cls_token = einops.repeat(cls_token, '() n e -> b n e', b=batch_size)\n",
    "print(f'Class Token after Repeat batch size : {cls_token.shape}')\n",
    "\n",
    "# position encoding\n",
    "# H position -> 224/16 =14  , W position -> 224/16 =14 , 14*14 만큼 position\n",
    "# class와 patch를 concat해서 하나가 더 생긴다. 그러므로 포지션도 1개더 만들어줌\n",
    "position = nn.Parameter(torch.randn((img_size//patch_size)**2+1, emb_size))\n",
    "print(f'Position : {position.shape}')\n",
    "\n",
    "# cls_token과 patch_x 를 concatenate\n",
    "concat_x = torch.cat([cls_token,patch_x], dim=1)\n",
    "print(f'concat x shape : {concat_x.shape}')\n",
    "\n",
    "# posistion 을 더해준다.\n",
    "concat_x += position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class로 Patch embedding 구현   \n",
    "-- Bool value of Tensor / tensor로 bool값으로 비교 하려 할때 나오는 error   \n",
    "-- patchEmbedding class를 선언하고 x에 적용해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=16, emb_size= 768, img_size=224):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size % patch_size ==0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, \n",
    "                emb_size, \n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size\n",
    "            ),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        self.cls_token =nn.Parameter(torch.randn(1,1,emb_size))\n",
    "        self.position = nn.Parameter(torch.randn((img_size//patch_size)**2+1, emb_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = einops.repeat(self.cls_token, '() n e -> b n e',b=b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.position\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch shape : torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "Patch_Embedding = PatchEmbedding()\n",
    "patch = Patch_Embedding(x)\n",
    "print(f'patch shape : {patch.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Attention   \n",
    "- Query, Key, Value 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n"
     ]
    }
   ],
   "source": [
    "emb_size = 768\n",
    "num_heads = 8\n",
    "\n",
    "query = nn.Linear(emb_size, emb_size)\n",
    "key = nn.Linear(emb_size,emb_size)\n",
    "value = nn.Linear(emb_size,emb_size)\n",
    "print(f'{query}\\n{key}\\n{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query(x) shape : torch.Size([8, 197, 768])\n",
      "query : torch.Size([8, 8, 197, 96]) \n",
      "key : torch.Size([8, 8, 197, 96])\n",
      "value : torch.Size([8, 8, 197, 96])\n"
     ]
    }
   ],
   "source": [
    "print(f'query(x) shape : {query(patch).shape}')\n",
    "query = einops.rearrange(query(patch), 'b n (h d) -> b h n d', h=num_heads)\n",
    "key = einops.rearrange(key(patch), 'b n (h d) -> b h n d', h=num_heads)\n",
    "value = einops.rearrange(value(patch), 'b n (h d) -> b h n d', h=num_heads)\n",
    "print(f'query : {query.shape} \\nkey : {key.shape}\\nvalue : {value.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재의 Query 에 대해 모든 Key값을 한번 씩 곱한다   \n",
    "Query * Key^T 에 Softmax한 확률   \n",
    "-> Softmax * value    \n",
    "\n",
    "- matmul 와 einsum 2가지 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape : torch.Size([8, 8, 197, 96])\n",
      "Key shape : torch.Size([8, 8, 197, 96])\n",
      "score shape : torch.Size([8, 8, 197, 197])\n",
      "score == score2 ? True\n",
      "\n",
      "scaling : 27.712812921102035\n",
      "attention : torch.Size([8, 8, 197, 197])\n",
      "Attention * value : torch.Size([8, 8, 197, 96])\n",
      "out == out2 ?: True\n",
      "output : torch.Size([8, 197, 768])\n",
      "patch와 동일한 크기가 나옴\n"
     ]
    }
   ],
   "source": [
    "# Query * Key\n",
    "print(f'Query shape : {query.shape}')\n",
    "print(f'Key shape : {key.shape}')\n",
    "score = torch.matmul(query,key.transpose(-1,-2))\n",
    "score2 = torch.einsum('bhqd, bhkd -> bhqk', query,key)\n",
    "print(f'score shape : {score2.shape}')\n",
    "print(f'score == score2 ? {(score==score2).all()}\\n')\n",
    "\n",
    "# Attention Score / emb_size 에 루트한 값을 나눈다\n",
    "scaling = emb_size ** (1/2)\n",
    "print(f'scaling : {scaling}')\n",
    "score /= scaling\n",
    "attention = torch.nn.functional.softmax(score, dim=-1)\n",
    "print(f'attention : {attention.shape}')\n",
    "\n",
    "# Attention score * value\n",
    "out = torch.matmul(attention, value)\n",
    "out2 = torch.einsum('bhal, bhlv -> bhav', attention, value)\n",
    "print(f'Attention * value : {out.shape}')\n",
    "print(f'out == out2 ?: {(out==out2).all()}')\n",
    "\n",
    "# Rearrange to emb_size\n",
    "out = einops.rearrange(out, 'b h n d -> b n (h d)')\n",
    "print(f'output : {out.shape}')\n",
    "print(f'patch와 동일한 크기가 나옴')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QKV 당 1개의 Linear Layer를 적용한 것을 텐서 연산을 한번에 하기위해   \n",
    "emb_size *3으로 설정한 후 각각 나누어 준다.   \n",
    "Attention 시 무시할 정보가 있을 경우 masking으로 하기 위해 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size=768, num_heads=8, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.qkv= nn.Linear(emb_size, emb_size *3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        self.scaling = (emb_size//num_heads)**(-1/2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        qkv = einops.rearrange(self.qkv(x),'b n ( h d qkv) -> (qkv) b h n d', h=self.num_heads, qkv=3)\n",
    "\n",
    "        query, key, value = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        score = torch.einsum('bhqd, bhkd -> bhqk', query,key)\n",
    "        #print(f'score shape : {score.shape}')\n",
    "\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            score.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        score= score * self.scaling\n",
    "\n",
    "        atten = torch.nn.functional.softmax(score,dim=-1)\n",
    "        atten = self.att_drop(atten)\n",
    "        #print(f'attention shape : {atten.shape}')\n",
    "\n",
    "        out = torch.einsum('bhal, bhlv -> bhav',atten, value)\n",
    "        out = einops.rearrange(out, 'b h n d -> b n (h d)')\n",
    "        #print(f'out shape : {out.shape}')\n",
    "        # 왜 마지막에 Linear 하는거지?\n",
    "        out = self.projection(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "Multihead = MultiHeadAttention()\n",
    "output = Multihead(patch)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self,fn):\n",
    "        super().__init__()\n",
    "        self.fn =fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x \n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int=4, drop_p : float =0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Encoder   \n",
    "patch embding -> MultiHead attention -> MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_size = 768,\n",
    "        drop_p = 0.,\n",
    "        forward_expansion = 4,\n",
    "        forward_drop_p = 0.,\n",
    "        **kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size,**kwargs),\n",
    "                nn.Dropout(drop_p))\n",
    "            ),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p\n",
    "                ),\n",
    "                nn.Dropout(drop_p))\n",
    "            )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes:int =1000):\n",
    "        super().__init__(\n",
    "            einops.layers.torch.Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        patch_size : int = 16,\n",
    "        emb_size : int = 768,\n",
    "        img_size : int =224,\n",
    "        depth: int = 12,\n",
    "        n_classes: int = 1000,\n",
    "        **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size = emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "         Rearrange-2             [-1, 196, 768]               0\n",
      "    PatchEmbedding-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6          [-1, 8, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-8             [-1, 197, 768]               0\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "      ResidualAdd-10             [-1, 197, 768]               0\n",
      "        LayerNorm-11             [-1, 197, 768]           1,536\n",
      "           Linear-12            [-1, 197, 3072]       2,362,368\n",
      "             GELU-13            [-1, 197, 3072]               0\n",
      "          Dropout-14            [-1, 197, 3072]               0\n",
      "           Linear-15             [-1, 197, 768]       2,360,064\n",
      "          Dropout-16             [-1, 197, 768]               0\n",
      "      ResidualAdd-17             [-1, 197, 768]               0\n",
      "        LayerNorm-18             [-1, 197, 768]           1,536\n",
      "           Linear-19            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-20          [-1, 8, 197, 197]               0\n",
      "           Linear-21             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-22             [-1, 197, 768]               0\n",
      "          Dropout-23             [-1, 197, 768]               0\n",
      "      ResidualAdd-24             [-1, 197, 768]               0\n",
      "        LayerNorm-25             [-1, 197, 768]           1,536\n",
      "           Linear-26            [-1, 197, 3072]       2,362,368\n",
      "             GELU-27            [-1, 197, 3072]               0\n",
      "          Dropout-28            [-1, 197, 3072]               0\n",
      "           Linear-29             [-1, 197, 768]       2,360,064\n",
      "          Dropout-30             [-1, 197, 768]               0\n",
      "      ResidualAdd-31             [-1, 197, 768]               0\n",
      "        LayerNorm-32             [-1, 197, 768]           1,536\n",
      "           Linear-33            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-34          [-1, 8, 197, 197]               0\n",
      "           Linear-35             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-36             [-1, 197, 768]               0\n",
      "          Dropout-37             [-1, 197, 768]               0\n",
      "      ResidualAdd-38             [-1, 197, 768]               0\n",
      "        LayerNorm-39             [-1, 197, 768]           1,536\n",
      "           Linear-40            [-1, 197, 3072]       2,362,368\n",
      "             GELU-41            [-1, 197, 3072]               0\n",
      "          Dropout-42            [-1, 197, 3072]               0\n",
      "           Linear-43             [-1, 197, 768]       2,360,064\n",
      "          Dropout-44             [-1, 197, 768]               0\n",
      "      ResidualAdd-45             [-1, 197, 768]               0\n",
      "        LayerNorm-46             [-1, 197, 768]           1,536\n",
      "           Linear-47            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-48          [-1, 8, 197, 197]               0\n",
      "           Linear-49             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-50             [-1, 197, 768]               0\n",
      "          Dropout-51             [-1, 197, 768]               0\n",
      "      ResidualAdd-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 3072]       2,362,368\n",
      "             GELU-55            [-1, 197, 3072]               0\n",
      "          Dropout-56            [-1, 197, 3072]               0\n",
      "           Linear-57             [-1, 197, 768]       2,360,064\n",
      "          Dropout-58             [-1, 197, 768]               0\n",
      "      ResidualAdd-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-62          [-1, 8, 197, 197]               0\n",
      "           Linear-63             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-64             [-1, 197, 768]               0\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "      ResidualAdd-66             [-1, 197, 768]               0\n",
      "        LayerNorm-67             [-1, 197, 768]           1,536\n",
      "           Linear-68            [-1, 197, 3072]       2,362,368\n",
      "             GELU-69            [-1, 197, 3072]               0\n",
      "          Dropout-70            [-1, 197, 3072]               0\n",
      "           Linear-71             [-1, 197, 768]       2,360,064\n",
      "          Dropout-72             [-1, 197, 768]               0\n",
      "      ResidualAdd-73             [-1, 197, 768]               0\n",
      "        LayerNorm-74             [-1, 197, 768]           1,536\n",
      "           Linear-75            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-76          [-1, 8, 197, 197]               0\n",
      "           Linear-77             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-78             [-1, 197, 768]               0\n",
      "          Dropout-79             [-1, 197, 768]               0\n",
      "      ResidualAdd-80             [-1, 197, 768]               0\n",
      "        LayerNorm-81             [-1, 197, 768]           1,536\n",
      "           Linear-82            [-1, 197, 3072]       2,362,368\n",
      "             GELU-83            [-1, 197, 3072]               0\n",
      "          Dropout-84            [-1, 197, 3072]               0\n",
      "           Linear-85             [-1, 197, 768]       2,360,064\n",
      "          Dropout-86             [-1, 197, 768]               0\n",
      "      ResidualAdd-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "           Linear-89            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-90          [-1, 8, 197, 197]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-92             [-1, 197, 768]               0\n",
      "          Dropout-93             [-1, 197, 768]               0\n",
      "      ResidualAdd-94             [-1, 197, 768]               0\n",
      "        LayerNorm-95             [-1, 197, 768]           1,536\n",
      "           Linear-96            [-1, 197, 3072]       2,362,368\n",
      "             GELU-97            [-1, 197, 3072]               0\n",
      "          Dropout-98            [-1, 197, 3072]               0\n",
      "           Linear-99             [-1, 197, 768]       2,360,064\n",
      "         Dropout-100             [-1, 197, 768]               0\n",
      "     ResidualAdd-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-104          [-1, 8, 197, 197]               0\n",
      "          Linear-105             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-106             [-1, 197, 768]               0\n",
      "         Dropout-107             [-1, 197, 768]               0\n",
      "     ResidualAdd-108             [-1, 197, 768]               0\n",
      "       LayerNorm-109             [-1, 197, 768]           1,536\n",
      "          Linear-110            [-1, 197, 3072]       2,362,368\n",
      "            GELU-111            [-1, 197, 3072]               0\n",
      "         Dropout-112            [-1, 197, 3072]               0\n",
      "          Linear-113             [-1, 197, 768]       2,360,064\n",
      "         Dropout-114             [-1, 197, 768]               0\n",
      "     ResidualAdd-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-118          [-1, 8, 197, 197]               0\n",
      "          Linear-119             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-120             [-1, 197, 768]               0\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "     ResidualAdd-122             [-1, 197, 768]               0\n",
      "       LayerNorm-123             [-1, 197, 768]           1,536\n",
      "          Linear-124            [-1, 197, 3072]       2,362,368\n",
      "            GELU-125            [-1, 197, 3072]               0\n",
      "         Dropout-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "     ResidualAdd-129             [-1, 197, 768]               0\n",
      "       LayerNorm-130             [-1, 197, 768]           1,536\n",
      "          Linear-131            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-132          [-1, 8, 197, 197]               0\n",
      "          Linear-133             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-134             [-1, 197, 768]               0\n",
      "         Dropout-135             [-1, 197, 768]               0\n",
      "     ResidualAdd-136             [-1, 197, 768]               0\n",
      "       LayerNorm-137             [-1, 197, 768]           1,536\n",
      "          Linear-138            [-1, 197, 3072]       2,362,368\n",
      "            GELU-139            [-1, 197, 3072]               0\n",
      "         Dropout-140            [-1, 197, 3072]               0\n",
      "          Linear-141             [-1, 197, 768]       2,360,064\n",
      "         Dropout-142             [-1, 197, 768]               0\n",
      "     ResidualAdd-143             [-1, 197, 768]               0\n",
      "       LayerNorm-144             [-1, 197, 768]           1,536\n",
      "          Linear-145            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-146          [-1, 8, 197, 197]               0\n",
      "          Linear-147             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-148             [-1, 197, 768]               0\n",
      "         Dropout-149             [-1, 197, 768]               0\n",
      "     ResidualAdd-150             [-1, 197, 768]               0\n",
      "       LayerNorm-151             [-1, 197, 768]           1,536\n",
      "          Linear-152            [-1, 197, 3072]       2,362,368\n",
      "            GELU-153            [-1, 197, 3072]               0\n",
      "         Dropout-154            [-1, 197, 3072]               0\n",
      "          Linear-155             [-1, 197, 768]       2,360,064\n",
      "         Dropout-156             [-1, 197, 768]               0\n",
      "     ResidualAdd-157             [-1, 197, 768]               0\n",
      "       LayerNorm-158             [-1, 197, 768]           1,536\n",
      "          Linear-159            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-160          [-1, 8, 197, 197]               0\n",
      "          Linear-161             [-1, 197, 768]         590,592\n",
      "MultiHeadAttention-162             [-1, 197, 768]               0\n",
      "         Dropout-163             [-1, 197, 768]               0\n",
      "     ResidualAdd-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 3072]       2,362,368\n",
      "            GELU-167            [-1, 197, 3072]               0\n",
      "         Dropout-168            [-1, 197, 3072]               0\n",
      "          Linear-169             [-1, 197, 768]       2,360,064\n",
      "         Dropout-170             [-1, 197, 768]               0\n",
      "     ResidualAdd-171             [-1, 197, 768]               0\n",
      "          Reduce-172                  [-1, 768]               0\n",
      "       LayerNorm-173                  [-1, 768]           1,536\n",
      "          Linear-174                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 364.33\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 694.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(ViT(), (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
