{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import einops\n",
    "from einops.layers.torch import Rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 224, 224])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(8,3,224,224)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Embedding   \n",
    "- 이미지를 Patch로 나누는 방법 2가지   \n",
    "-- 1. einops의 rearrange   \n",
    "-- 2. Covn2d layer로 patch크기와 같은 filter를 사용   \n",
    "   \n",
    "   Batch * C * H * W --> Batch * N * (P * P * C)   \n",
    "   H * W --> N * ( P * P )   \n",
    "- 실제의 VIT에서는 einops같은 Linear Embedding 보다 Conv2d Layer로 사용한 후 Flatten 한 것이 performance gain이 있습니다   \n",
    "   \n",
    "   -- google research 에서는 conv2d 후 jax.numpy로 reshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n",
      "patches : torch.Size([8, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "# 1. einops.rearrange 함수로 patch\n",
    "# 8x3x(14*16)x(14*16) -> 8x(14*14)x(16*16*3) 으로 flatten\n",
    "patch_size = 16\n",
    "\n",
    "print(x.shape)\n",
    "patches = einops.rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n",
    "print(f'patches : {patches.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. convlayer로 patch 만들기\n",
    "patch_size = 16\n",
    "in_channels =3\n",
    "emb_size = 768\n",
    "\n",
    "projection = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, emb_size, kernel_size= patch_size,\n",
    "    stride=patch_size),\n",
    "    # einops.layers.torch.Rearrange 함수 사용\n",
    "    Rearrange('b e (h) (w) -> b(h w) e')\n",
    ")\n",
    "projection(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class_Token과 Positional Encoding 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch x shape : torch.Size([8, 196, 768])\n",
      "Class Token shape : torch.Size([1, 1, 768])\n",
      "Class Token after Repeat batch size : torch.Size([8, 1, 768])\n",
      "Position : torch.Size([197, 768])\n",
      "concat x shape : torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "emb_size = 768\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "# patch만들기\n",
    "patch_x = projection(x)\n",
    "print(f'Patch x shape : {patch_x.shape}')\n",
    "\n",
    "#class token\n",
    "cls_token = nn.Parameter(torch.randn(1,1,emb_size))\n",
    "print(f'Class Token shape : {cls_token.shape}')\n",
    "\n",
    "batch_size = 8\n",
    "cls_token = einops.repeat(cls_token, '() n e -> b n e', b=batch_size)\n",
    "print(f'Class Token after Repeat batch size : {cls_token.shape}')\n",
    "\n",
    "# position encoding\n",
    "# H position -> 224/16 =14  , W position -> 224/16 =14 , 14*14 만큼 position\n",
    "# class와 patch를 concat해서 하나가 더 생긴다. 그러므로 포지션도 1개더 만들어줌\n",
    "position = nn.Parameter(torch.randn((img_size//patch_size)**2+1, emb_size))\n",
    "print(f'Position : {position.shape}')\n",
    "\n",
    "# cls_token과 patch_x 를 concatenate\n",
    "concat_x = torch.cat([cls_token,patch_x], dim=1)\n",
    "print(f'concat x shape : {concat_x.shape}')\n",
    "\n",
    "# posistion 을 더해준다.\n",
    "concat_x += position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class로 Patch embedding 구현   \n",
    "-- Bool value of Tensor / tensor로 bool값으로 비교 하려 할때 나오는 error   \n",
    "-- patchEmbedding class를 선언하고 x에 적용해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=16, emb_size= 768, img_size=224):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size % patch_size ==0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size,stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.cls_token =nn.Parameter(torch.randn(1,1,emb_size))\n",
    "        self.position = nn.Parameter(torch.randn((img_size//patch_size)**2+1, emb_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = einops.repeat(self.cls_token, '() n e -> b n e',b=b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.position\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch shape : torch.Size([8, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "Patch_Embedding = PatchEmbedding()\n",
    "patch = Patch_Embedding(x)\n",
    "print(f'patch shape : {patch.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Attention   \n",
    "- Query, Key, Value 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True) Linear(in_features=768, out_features=768, bias=True)\n"
     ]
    }
   ],
   "source": [
    "emb_size = 768\n",
    "num_heads = 8\n",
    "\n",
    "query = nn.Linear(emb_size, emb_size)\n",
    "key = nn.Linear(emb_size,emb_size)\n",
    "value = nn.Linear(emb_size,emb_size)\n",
    "print(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query(x) shape : torch.Size([8, 197, 768])\n",
      "query : torch.Size([8, 8, 197, 96]) \n",
      "key : torch.Size([8, 8, 197, 96])\n",
      "value : torch.Size([8, 8, 197, 96])\n"
     ]
    }
   ],
   "source": [
    "print(f'query(x) shape : {query(patch).shape}')\n",
    "query = einops.rearrange(query(patch), 'b n (h d) -> b h n d', h=num_heads)\n",
    "key = einops.rearrange(key(patch), 'b n (h d) -> b h n d', h=num_heads)\n",
    "value = einops.rearrange(value(patch), 'b n (h d) -> b h n d', h=num_heads)\n",
    "print(f'query : {query.shape} \\nkey : {key.shape}\\nvalue : {value.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재의 Query 에 대해 모든 Key값을 한번 씩 곱한다   \n",
    "Query * Key^T 에 Softmax한 확률   \n",
    "-> Softmax * value    \n",
    "\n",
    "- matmul 와 einsum 2가지 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape : torch.Size([8, 8, 197, 96])\n",
      "Key shape : torch.Size([8, 8, 197, 96])\n",
      "score shape : torch.Size([8, 8, 197, 197])\n",
      "score == score2 ? True\n",
      "\n",
      "scaling : 27.712812921102035\n",
      "attention : torch.Size([8, 8, 197, 197])\n",
      "Attention * value : torch.Size([8, 8, 197, 96])\n",
      "out == out2 ?: True\n",
      "output : torch.Size([8, 197, 768])\n",
      "patch와 동일한 크기가 나옴\n"
     ]
    }
   ],
   "source": [
    "# Query * Key\n",
    "print(f'Query shape : {query.shape}')\n",
    "print(f'Key shape : {key.shape}')\n",
    "score = torch.matmul(query,key.transpose(-1,-2))\n",
    "score2 = torch.einsum('bhqd, bhkd -> bhqk', query,key)\n",
    "print(f'score shape : {score2.shape}')\n",
    "print(f'score == score2 ? {(score==score2).all()}\\n')\n",
    "\n",
    "# Attention Score / emb_size 에 루트한 값을 나눈다\n",
    "scaling = emb_size ** (1/2)\n",
    "print(f'scaling : {scaling}')\n",
    "score /= scaling\n",
    "attention = torch.nn.functional.softmax(score, dim=-1)\n",
    "print(f'attention : {attention.shape}')\n",
    "\n",
    "# Attention score * value\n",
    "out = torch.matmul(attention, value)\n",
    "out2 = torch.einsum('bhal, bhlv -> bhav', attention, value)\n",
    "print(f'Attention * value : {out.shape}')\n",
    "print(f'out == out2 ?: {(out==out2).all()}')\n",
    "\n",
    "# Rearrange to emb_size\n",
    "out = einops.rearrange(out, 'b h n d -> b n (h d)')\n",
    "print(f'output : {out.shape}')\n",
    "print(f'patch와 동일한 크기가 나옴')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QKV 당 1개의 Linear Layer를 적용한 것을 텐서 연산을 한번에 하기위해   \n",
    "emb_size *3으로 설정한 후 각각 나누어 준다.   \n",
    "Attention 시 무시할 정보가 있을 경우 masking으로 하기 위해 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size=768, num_heads=8, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.qkv= nn.Linear(emb_size, emb_size *3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        self.scaling = emb_size**(1/2)\n",
    "    def forward(self, x, mask=None):\n",
    "        qkv = einops.rearrange(self.qkv(x),'b n ( h d qkv) -> (qkv) b h n d', h=self.num_heads, qkv=3)\n",
    "\n",
    "        query, key, value = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        score = torch.einsum('bhqd, bhkd -> bhqk', query,key)\n",
    "\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            score.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        score= score * self.scaling\n",
    "        atten = torch.nn.functional.softmax(score,dim=-1)\n",
    "        atten = self.att_drop(atten)\n",
    "\n",
    "        out = torch.einsum('bhal, bhlv -> bhav',atten, value)\n",
    "        out = einops.rearrange(out, 'b h n d -> b n (h d)')\n",
    "        # 왜 마지막에 Linear 하는거지?\n",
    "        out = self.projection(out)\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
